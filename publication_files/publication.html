<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="Hi, I'm Anahita! I'm passionate about leveraging AI in real-world applications to drive innovation and efficiency.">
        <title>Anahita's homepage!</title>
        <link rel="stylesheet" href="/publication_files/publication_styles.css">
        <script src="/publication_files/publication_script.js" defer></script>

    </head>
    <body>
        <header>
            <div id="Tabs" class="tabs">
                <div></div>
                <div class="tab-Anahita"><a href="/index.html" style="color: #b76e79; text-decoration:none;">Anahita</a></div>
                <div></div>
                <div class="tab-all">
                    <div class="tab-each"> <a href="/education_files/education.html" style="color: #222; text-decoration:none;">Education</a></div>
                    <div class="tab-each"><a href="/collaboration_files/collaboration.html" style="color: #222; text-decoration:none;">Collaborations</a></div>
                    <!--<div class="tab-each"><a href="/project_files/project.html" style="color: #222; text-decoration:none;">Projects</a></div>-->
                    <div class="tab-each"><a href="/publication_files/publication.html" style="color: #222; text-decoration:none;">Publications</a></div>
                    <div class="tab-each"><a href="/skill_files/skill.html" style="color: #222; text-decoration:none;">Skills</a></div>
                    <!--<div>Certificates</div>-->
                    <!--<div>Awards</div>-->
                    <div class="tab-each"><a href="/about_me_files/about_me.html" style="color: #222; text-decoration:none;">About</a></div>
                </div>
                <div></div>
            </div>
        </header>

        <main class="publication-page">
            <section class="media-gallery" aria-label="Publications carousel">

            <div class="media-gallery-viewport" role="region" aria-roledescription="carousel" aria-label="Publications carousel">
                <div class="media-gallery-track" id="mediaGalleryTrack">
                <article class="media-gallery-item publication-card">
                    <figure class="publication-figure">
                        <img src="_VNN.jpeg" alt="VNN: Verification-Friendly Neural Networks with Hard Robustness Guarantees">
                    </figure>
                    <h2 class="publication-title">
                    <strong>VNN: Verification-Friendly Neural Networks with Hard Robustness Guarantees</strong>
                    </h2>
                    <p class="publication-venue">
                    Presented at <em>The International Conference on Machine Learning (ICML) 2024</em>
                    </p>
                    <p class="publication-text">
                    Formal verification of neural networks often struggles to scale, limiting practical robustness guarantees.
                    This work introduces a post-training approach that transforms existing models into verification-friendly networks without sacrificing accuracy.
                    As a result, robustness can be established faster and for significantly more inputs.
                    </p>
                    <div class="publication-actions">
                    <a class="publication-btn" href="https://openreview.net/pdf?id=gUFufRkzjV" target="_blank" rel="noopener">PDF</a>
                    <a class="publication-btn" href="https://github.com/anahitabn94/VNN" target="_blank" rel="noopener">Code</a>
                    </div>
                </article>

                <article class="media-gallery-item publication-card">
                    <figure class="publication-figure">
                        <img src="_SafeDeep.jpeg" alt="SafeDeep: A Scalable Robustness Verification Framework for Deep Neural Networks">
                    </figure>
                    <h2 class="publication-title">
                    <strong>SafeDeep: A Scalable Robustness Verification Framework for Deep Neural Networks</strong>
                    </h2>
                    <p class="publication-venue">
                    Presented at <em>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2023</em>
                    </p>
                    <p class="publication-text">
                    Deep neural networks are increasingly used in safety-critical systems, yet their behavior under small perturbations remains difficult to guarantee.
                    This paper introduces SafeDeep, a verification framework designed to deliver tighter bounds for stronger robustness guarantees.
                    By bridging the gap between theory and practice, the work shows how formal verification can become a practical tool for deploying trustworthy deep learning systems.
                    </p>
                    <div class="publication-actions">
                    <a class="publication-btn" href="https://ieeexplore.ieee.org/abstract/document/10097028" target="_blank" rel="noopener">PDF</a>
                    <a class="publication-btn" href="https://github.com/anahitabn94/SafeDeep" target="_blank" rel="noopener">Code</a>
                    </div>
                </article>

                <article class="media-gallery-item publication-card">
                    <figure class="publication-figure">
                        <img src="_FLI.jpeg" alt="Formal Local Implication Between Two Neural Networks">
                    </figure>
                    <h2 class="publication-title">
                    <strong>Formal Local Implication Between Two Neural Networks</strong>
                    </h2>
                    <p class="publication-venue">
                    Presented at <em>European Conference on Artificial Intelligence (ECAI) 2025</em>
                    </p>
                    <p class="publication-text">
                    Modern AI systems often rely on compact or modified neural networks, yet it remains unclear whether these models behave safely relative to their original versions.
                    This paper introduces formal local implication, a new verification concept that proves one network makes a correct decision whenever another does, across an entire input region.
                    The approach enables rigorous comparison of original and compact models, providing stronger safety assurances for deploying neural networks in resource-constrained and medical applications
                    </p>
                    <div class="publication-actions">
                    <a class="publication-btn" href="https://ebooks.iospress.nl/volumearticle/75864" target="_blank" rel="noopener">PDF</a>
                    <a class="publication-btn" href="https://github.com/anahitabn94/Formal_Local_Implication" target="_blank" rel="noopener">Code</a>
                    </div>
                </article>

                <article class="media-gallery-item publication-card">
                    <figure class="publication-figure">
                        <img src="_privacy.jpeg" alt="Robustness and Privacy Interplay in Patient Membership Inference">
                    </figure>
                    <h2 class="publication-title">
                    <strong>Robustness and Privacy Interplay in Patient Membership Inference</strong>
                    </h2>
                    <p class="publication-venue">
                    Presented at <em>IEEE International Joint Conference on Neural Networks (IJCNN) 2025</em>
                    </p>
                    <p class="publication-text">
                    Modern personalized deep learning models, especially in healthcare, face hidden privacy risks that go beyond standard membership inference attacks because patient identity can be inferred even without access to training data.
                    This paper reveals a deep connection between a model’s robustness and its vulnerability to patient membership inference, and proposes a framework that exploits this interplay to identify which patient’s data was used to train a model.
                    By evaluating real biomedical applications like epileptic seizure and cardiac arrhythmia detection, the work shows that privacy and safety are fundamentally linked.
                    </p>
                    <div class="publication-actions">
                    <a class="publication-btn" href="https://ieeexplore.ieee.org/document/11227914" target="_blank" rel="noopener">PDF</a>
                    </div>
                </article>

                <!--<article class="media-gallery-item publication-card">
                    <figure class="publication-figure">
                        <img src="https://picsum.photos/id/1045/1200/800" alt="Preview of Article 5">
                    </figure>
                    <h2 class="publication-title">Article 5 title</h2>
                    <p class="publication-text">
                    Short summary of the article. Replace this text with your abstract or a one paragraph description.
                    </p>
                    <div class="publication-actions">
                    <a class="publication-btn" href="#" target="_blank" rel="noopener">PDF</a>
                    <a class="publication-btn" href="#" target="_blank" rel="noopener">Code</a>
                    </div>
                </article>-->
                </div>

                <!--<div class="media-gallery-controls" aria-label="Carousel controls">
                <button class="media-gallery-btn" type="button" id="mediaPrevBtn" aria-label="Previous item">Previous</button>
                <button class="media-gallery-btn" type="button" id="mediaPauseBtn" aria-label="Pause autoplay" aria-pressed="false">Pause</button>
                <button class="media-gallery-btn" type="button" id="mediaNextBtn" aria-label="Next item">Next</button>
                </div>-->
                <div class="media-dots" id="mediaDots" aria-label="Carousel navigation">
                    <button class="media-dot is-active" type="button" aria-label="Go to item 1"></button>
                    <button class="media-dot" type="button" aria-label="Go to item 2"></button>
                    <button class="media-dot" type="button" aria-label="Go to item 3"></button>
                    <button class="media-dot" type="button" aria-label="Go to item 4"></button>
                    <!--<button class="media-dot" type="button" aria-label="Go to item 5"></button>-->
                </div>

            </div>
            <div class="media-gallery-header">
                <h1 class="media-gallery-title">Full list of my publications is available on: 
                    <a href="https://scholar.google.com/citations?user=s1QPqrkAAAAJ&hl=en&oi=sra" target="_blank" rel="noopener noreferrer"  style="text-decoration: none;"> 
                        my google scholar profile
                    </a>
                </h1>
            </div>
            </section>
        </main>
    </body>
</html>
